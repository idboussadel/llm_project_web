"""
Analyseur de Sentiment Llama-2 pour SentiTrade-HMA-V2
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel
import json
from pathlib import Path
import pandas as pd
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')


class LlamaSentimentAnalyzer:
    """
    Analyseur de sentiment financier - Llama-2 fine-tun√© avec LoRA
    
    Performances :
        - MCC interne: 0.9926
        - MCC externe: 0.8865  
        - Accuracy externe: 92.22%
    """
    
    def __init__(self, model_path: str = None, device: str = "auto"):
        """
        Args:
            model_path: Chemin vers le mod√®le LoRA
            device: 'cuda', 'cpu' ou 'auto'
        """
        if model_path is None:
            model_path = self._find_model_path()
        
        self.model_path = Path(model_path)
        
        # Device
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print("="*80)
        print("ü¶ô LLAMA-2 SENTIMENT ANALYZER")
        print("="*80)
        print(f"Device: {self.device}")
        print(f"Mod√®le: {self.model_path}")
        
        # Charger config
        config_path = self.model_path.parent / "model_config.json"
        if not config_path.exists():
            # Chercher un niveau au-dessus si dans fold0_final
            config_path = self.model_path.parent.parent / "model_config.json"
        
        if not config_path.exists():
            raise FileNotFoundError(f"‚ùå Configuration non trouv√©e: {config_path}")
        
        with open(config_path) as f:
            self.config = json.load(f)
        
        self.label_mapping = self.config["label_mapping"]
        self.id_to_label = {int(k): v for k, v in self.label_mapping.items()}
        self.label_to_id = {v: int(k) for k, v in self.label_mapping.items()}
        
        print(f"Performance: MCC {self.config['metrics']['mcc_external']:.4f}")
        
        # Tokenizer
        print("\nüì• Chargement tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config["model_name"],
            use_fast=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        print("‚úÖ Tokenizer charg√©")
        
        # Mod√®le base
        print("\nüì• Chargement Llama-2-7B...")
        print("   (1√®re fois: t√©l√©chargement ~13GB, ~5 min)")
        print("   (Suivantes: cache local, ~30 sec)")
        
        torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
        
        try:
            base_model = AutoModelForSequenceClassification.from_pretrained(
                self.config["model_name"],
                num_labels=self.config["num_labels"],
                id2label=self.id_to_label,
                label2id=self.label_to_id,
                torch_dtype=torch_dtype,
                low_cpu_mem_usage=True,
                device_map=self.device if self.device == "cpu" else "auto"
            )
            print("‚úÖ Mod√®le base charg√©")
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è Erreur: {e}")
            print("\nüí° Solution si mod√®le Llama-2 n√©cessite autorisation:")
            print("   1. Va sur huggingface.co/meta-llama/Llama-2-7b-hf")
            print("   2. Accepte les conditions")
            print("   3. Cr√©e un token: huggingface.co/settings/tokens")
            print("   4. Execute: huggingface-cli login")
            raise
        
        # LoRA
        print("\nüì• Chargement adaptateurs LoRA...")
        
        try:
            self.model = PeftModel.from_pretrained(base_model, str(self.model_path))
            self.model = self.model.to(self.device)
            self.model.eval()
            print("‚úÖ LoRA charg√©")
            
        except Exception as e:
            print(f"\n‚ùå Erreur LoRA: {e}")
            print(f"\nüí° V√©rifie ces fichiers:")
            print(f"   - {self.model_path / 'adapter_config.json'}")
            print(f"   - {self.model_path / 'adapter_model.safetensors'}")
            raise
        
        print("\n" + "="*80)
        print("‚úÖ MOD√àLE PR√äT")
        print("="*80)
    
    def _find_model_path(self) -> Path:
        """Auto-d√©tecte le chemin du mod√®le"""
        possible_paths = [
            # Si dans fold0_final
            Path("models/sentiment_llm/llama2_sentiment_lora/fold0_final"),
            # Si renomm√©
            Path("models/sentiment_llm/llama2_sentiment_lora"),
            # Chemins relatifs
            Path("../models/sentiment_llm/llama2_sentiment_lora/fold0_final"),
            Path("../../models/sentiment_llm/llama2_sentiment_lora/fold0_final"),
        ]
        
        for path in possible_paths:
            if path.exists() and (path / "adapter_config.json").exists():
                return path
        
        raise FileNotFoundError(
            "‚ùå Mod√®le non trouv√©.\n"
            "V√©rifie que adapter_config.json existe dans :\n"
            "  models/sentiment_llm/llama2_sentiment_lora/fold0_final/"
        )
    
    def predict(self, text: str, return_probs: bool = False) -> Dict:
        """
        Pr√©dit le sentiment d'un texte
        
        Args:
            text: Phrase financi√®re √† analyser
            return_probs: Retourner les probabilit√©s d√©taill√©es
        
        Returns:
            {
                'label': 'positive' | 'negative' | 'neutral',
                'score': -1 | 0 | 1,
                'confidence': float (0-1),
                'probabilities': dict (optionnel)
            }
        """
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            pred_id = torch.argmax(probs, dim=-1).item()
            confidence = probs[0][pred_id].item()
        
        label = self.id_to_label[pred_id]
        
        result = {
            "label": label,
            "score": pred_id - 1,  # -1, 0, 1
            "confidence": confidence
        }
        
        if return_probs:
            result["probabilities"] = {
                "negative": probs[0][0].item(),
                "neutral": probs[0][1].item(),
                "positive": probs[0][2].item()
            }
        
        return result
    
    def predict_batch(self, texts: List[str], batch_size: int = 16) -> List[Dict]:
        """Pr√©diction batch (plus rapide)"""
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                pred_ids = torch.argmax(probs, dim=-1)
                confidences = probs[range(len(pred_ids)), pred_ids]
            
            for pred_id, conf in zip(pred_ids, confidences):
                pred_id_item = pred_id.item()
                label = self.id_to_label[pred_id_item]
                results.append({
                    "label": label,
                    "score": pred_id_item - 1,
                    "confidence": conf.item()
                })
        
        return results
    
    def predict_dataframe(self, df: pd.DataFrame, text_column: str = "sentence", 
                         batch_size: int = 16) -> pd.DataFrame:
        """
        Ajoute sentiments √† un DataFrame
        
        Colonnes ajout√©es:
            - sentiment: label
            - sentiment_score: -1/0/1
            - sentiment_confidence: 0-1
        """
        print(f"\nüìä Analyse de {len(df)} lignes...")
        
        texts = df[text_column].tolist()
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_results = self.predict_batch(texts[i:i+batch_size], batch_size)
            results.extend(batch_results)
            
            if (i + batch_size) % 100 == 0 or (i + batch_size) >= len(texts):
                processed = min(i + batch_size, len(texts))
                print(f"   {processed}/{len(texts)} ({100*processed/len(texts):.1f}%)")
        
        df = df.copy()
        df['sentiment'] = [r['label'] for r in results]
        df['sentiment_score'] = [r['score'] for r in results]
        df['sentiment_confidence'] = [r['confidence'] for r in results]
        
        print(f"   ‚úÖ Termin√©")
        print(f"\nüìà Distribution: {df['sentiment'].value_counts().to_dict()}")
        print(f"üìä Confiance moy: {df['sentiment_confidence'].mean():.4f}")
        
        return df


def load_sentiment_analyzer(model_path: str = None, device: str = "auto"):
    """
    Helper pour charger facilement
    
    Usage:
        from src.sentiment import load_sentiment_analyzer
        
        analyzer = load_sentiment_analyzer()
        result = analyzer.predict("Strong earnings")
    """
    return LlamaSentimentAnalyzer(model_path=model_path, device=device)


# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    print("\nüß™ TEST LLAMA-2 SENTIMENT\n")
    
    try:
        analyzer = load_sentiment_analyzer()
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        exit(1)
    
    # Test simple
    test = "The company reported strong quarterly earnings"
    result = analyzer.predict(test, return_probs=True)
    
    print(f"\nTexte: '{test}'")
    print(f"Sentiment: {result['label']}")
    print(f"Score: {result['score']}")
    print(f"Confiance: {result['confidence']:.4f}")
    print(f"\nProbabilit√©s:")
    for label, prob in result['probabilities'].items():
        print(f"  {label:8s}: {prob:.4f}")
    
    # Test batch
    print("\n" + "="*80)
    test_batch = [
        "Revenue exceeded expectations",
        "The company faces challenges",
        "Results were in line with forecasts"
    ]
    
    results = analyzer.predict_batch(test_batch)
    print(f"\nBatch de {len(test_batch)} phrases:")
    for text, res in zip(test_batch, results):
        print(f"  {res['label']:8s} ({res['confidence']:.3f}) - {text}")
    
    print("\n‚úÖ TOUS LES TESTS R√âUSSIS")
